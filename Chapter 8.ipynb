{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go back to the mnist dataset and try it with a three layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "relu = lambda x: (x>=0) * x\n",
    "relu2deriv = lambda x: x>=0\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 35, 40, 784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I: 34 Error: 0.480 Correct: 0.716"
     ]
    }
   ],
   "source": [
    "# Training iterations through the entire dataset\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2 # Multiply by 1/%turnoff \n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "        \n",
    "        error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "        \n",
    "        layer_2_delta = labels[i:i+1] - layer_2\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        \n",
    "        layer_1_delta *= dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    sys.stdout.write(\"\\r\" + \\\n",
    "                    \" I: \" + str(j) + \\\n",
    "                    \" Error: \" + str(error/float(len(images)))[0:5] + \\\n",
    "                    \" Correct: \" + str(correct_cnt/float(len(images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test-Error:  0.445  Test-Acc:  0.795\n"
     ]
    }
   ],
   "source": [
    "error, correct_cnt = (0.0, 0)\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    layer_0 = test_images[i:i+1]\n",
    "    layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "    layer_2 = layer_1.dot(weights_1_2)\n",
    "    \n",
    "    error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "    correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "print(\" Test-Error: \", str(error/float(len(test_images)))[0:5], \" Test-Acc: \", correct_cnt/float(len(test_images)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest regularization is early stopping. Regularixation is a subset of methods used to encourage generalization in learned models, often by incresing the difficulty for a model to learn the fine-grained details of the training data.\n",
    "\n",
    "Its helps the neural network learn the signal and ignore the noise of a dataset.\n",
    "\n",
    "Validation set is a set of data that isnt inside of the training set, or the test set, used to validate the accuracy of the network as it trains. \n",
    "\n",
    "Another regularization method is dropout. This method works by turning off neurons at random during training, i.e setting them to 0. This might be one of the most generally accepted go-to regularization methods. On a simple level, this works because smaller networks can capture less detail and thus captures less noise and more expressive details. \n",
    "\n",
    "At the same time, smaller networks can overfit to a dataset. But its highly unlikely that two networks will overfit to the same noise and the average between every network will be more generalized. \n",
    "\n",
    "Neural networks, even though they're randomly generated, still start by learning the biggest, most broadly sweeping features before learning much about the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1\n",
      "  0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "print(dropout_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rewrite the neural network with batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d72380ce8213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlayer_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlayer_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x >= 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0\n",
    "\n",
    "batch_size = 100\n",
    "alpha, iterations = (0.001, 300)\n",
    "pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    for i in range(len(images) // batch_size):\n",
    "        batch_start, batch_end = (i * batch_size, (i+1) * batch_size)\n",
    "        \n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = layer_0.dot(weights_0_1)\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "        \n",
    "        error += np.sum((labels[batch_start,batch_end] - layer_2) ** 2)\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k,k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "            \n",
    "            layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "            \n",
    "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "            \n",
    "    if j%10 == 0:\n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "        \n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "            \n",
    "            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "            \n",
    "        print(\"Err: \", error, \" Cnt: \", correct_cnt, \" Test Err: \", test_error, \" Test Cnt: \", test_correct_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
