{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions - Som constraints\n",
    "* The function should be continuous and have an infinite domain\n",
    "* The function should be monotonic - not required but good\n",
    "* Good activation functions are nonlinear\n",
    "\n",
    "Some good and fast activation functions are:\n",
    "* Sigmoid function - smoothly maps the infinite amount of inputs to an output between 0 and 1. This is useful because the output can be interperated as a probability, thus nonlinearity can be used in both hidden and output layers.\n",
    "* Tanh is also a good function, it gives values between -1 and 1. This is better than sigmoid in most cases for hidden layers.\n",
    "\n",
    "Activation functions on the output layer is highly dependent on what values you're predicting. \n",
    "\n",
    "When predicting raw data values, like temperature, it might be best to not use any activation functions on the output layer. \n",
    "\n",
    "When predicting unrelated yes/no probabilities (binary) its often useful to use a sigmoid activation function on the output. \n",
    "\n",
    "When predicting which-one, predicting a single label out of many like the MNIST classifier, its common to use softmax. Softmax helps the network to predict one output node much higher than the others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
